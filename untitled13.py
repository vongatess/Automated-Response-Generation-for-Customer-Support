# -*- coding: utf-8 -*-
"""Untitled13.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1rrl0UfcmBy0bmR8wcXSsb0u4IeVBHJC2
"""

!pip install transformers

pip install datasets

pip install Torch

from datasets import load_dataset

dataset = load_dataset("csv", data_files="Customer-Support.csv") # Use the 'csv' builder to load from a local file

from datasets import load_dataset
from datasets import Dataset
from transformers import GPT2Tokenizer
from sklearn.model_selection import train_test_split


print(dataset['train'][0])


tokenizer = GPT2Tokenizer.from_pretrained("gpt2")


tokenizer.pad_token = tokenizer.eos_token


train_dataset, test_dataset = train_test_split(dataset['train'], test_size=0.1)


train_dataset = Dataset.from_dict(train_dataset)
test_dataset = Dataset.from_dict(test_dataset)


def preprocess_function(examples):
    inputs = ["<|startoftext|> " + query for query in examples['query']]
    targets = [response + " <|endoftext|>" for response in examples['response']]
    model_inputs = tokenizer(inputs, max_length=512, truncation=True, padding="max_length")
    labels = tokenizer(targets, max_length=512, truncation=True, padding="max_length")

    model_inputs["labels"] = labels["input_ids"]
    return model_inputs


tokenized_train_dataset = train_dataset.map(preprocess_function, batched=True)
tokenized_test_dataset = test_dataset.map(preprocess_function, batched=True)

from transformers import GPT2LMHeadModel, Trainer, TrainingArguments


model = GPT2LMHeadModel.from_pretrained("gpt2")


training_args = TrainingArguments(
    output_dir="./results",
    evaluation_strategy="epoch",
    learning_rate=5e-5,
    per_device_train_batch_size=2,
    per_device_eval_batch_size=2,
    num_train_epochs=3,
    weight_decay=0.01,
    save_total_limit=2,
)


trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_train_dataset,
    eval_dataset=tokenized_test_dataset
)


trainer.train()

results = trainer.evaluate()

print(f"Perplexity: {results['eval_loss']}")


def generate_response(query):
    inputs = tokenizer("<|startoftext|> " + query, return_tensors="pt", max_length=512, truncation=True, padding=True)
    outputs = model.generate(inputs['input_ids'], max_length=512, num_return_sequences=1)
    response = tokenizer.decode(outputs[0], skip_special_tokens=True)
    return response


sample_query = "How can I reset my password?"
response = generate_response(sample_query)
print(f"Query: {sample_query}")
print(f"Response: {response}")